{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T18:29:51.661926Z",
     "iopub.status.busy": "2021-08-22T18:29:51.661659Z",
     "iopub.status.idle": "2021-08-22T18:29:51.956035Z",
     "shell.execute_reply": "2021-08-22T18:29:51.955292Z",
     "shell.execute_reply.started": "2021-08-22T18:29:51.661900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import addict\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pyproj\n",
    "# Global constants\n",
    "GEOID = pyproj.Geod(ellps=\"WGS84\")\n",
    "KM2M = 1.0e3\n",
    "M2MM = 1.0e3\n",
    "RADIUS_EARTH = np.float64((GEOID.a + GEOID.b) / 2)\n",
    "\n",
    "import celeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data files, create storage dictionaries, and do basic processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Western North America example\n",
    "# command_file_name = \"../data/command/western_north_america_command.json\"\n",
    "# Japan model\n",
    "command_file_name = \"../data/command/japan_command.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T18:29:53.570589Z",
     "iopub.status.busy": "2021-08-22T18:29:53.570331Z",
     "iopub.status.idle": "2021-08-22T18:30:09.817977Z",
     "shell.execute_reply": "2021-08-22T18:30:09.817443Z",
     "shell.execute_reply.started": "2021-08-22T18:29:53.570561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-19 17:12:05.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mget_logger\u001b[0m:\u001b[36m6486\u001b[0m - \u001b[1mRead: ../data/command/japan_command.json\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mget_logger\u001b[0m:\u001b[36m6487\u001b[0m - \u001b[1mRUN_NAME: 0000000069\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mget_logger\u001b[0m:\u001b[36m6488\u001b[0m - \u001b[1mWrite log file: ../runs/0000000069/0000000069.log\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m341\u001b[0m - \u001b[1mReading data files\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.486\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m345\u001b[0m - \u001b[32m\u001b[1mRead: ../data/segment/qp_japan_segment.csv\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.488\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m350\u001b[0m - \u001b[32m\u001b[1mRead: ../data/block/japan_block.csv\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.490\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m357\u001b[0m - \u001b[32m\u001b[1mRead: ../data/mesh/japan_mesh_parameters.json\u001b[0m\n",
      "\n",
      "\u001b[32m2024-08-19 17:12:05.561\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m512\u001b[0m - \u001b[32m\u001b[1mRead: ../data/mesh/nankai.msh\u001b[0m\n",
      "\n",
      "\u001b[32m2024-08-19 17:12:05.590\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m512\u001b[0m - \u001b[32m\u001b[1mRead: ../data/mesh/japan.msh\u001b[0m\n",
      "\n",
      "\u001b[32m2024-08-19 17:12:05.626\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m512\u001b[0m - \u001b[32m\u001b[1mRead: ../data/mesh/sagami.msh\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.630\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m548\u001b[0m - \u001b[32m\u001b[1mRead: ../data/station/japan_station.csv\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m563\u001b[0m - \u001b[1mNo mogi_file_name\u001b[0m\n",
      "\u001b[32m2024-08-19 17:12:05.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mceleri.celeri\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m585\u001b[0m - \u001b[1mNo sar_file_name\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "command = celeri.get_command(command_file_name)\n",
    "logger = celeri.get_logger(command)\n",
    "segment, block, meshes, station, mogi, sar = celeri.read_data(command)\n",
    "# Update mesh_parameters list\n",
    "with open(command.mesh_parameters_file_name) as f:\n",
    "    mesh_param = json.load(f)\n",
    "# Get mesh directory\n",
    "mesh_dir = os.path.dirname(mesh_param[0][\"mesh_filename\"])\n",
    "# Get stem of segment file name\n",
    "seg_file_stem = os.path.splitext(os.path.basename(command.segment_file_name))[0]\n",
    "n_meshes = len(meshes) # Number of preexisting meshes \n",
    "station = celeri.process_station(station, command)\n",
    "segment = celeri.process_segment(segment, command, meshes)\n",
    "closure, block = celeri.assign_block_labels(segment, station, block, mogi, sar)\n",
    "# Returning a copy of the closure class lets us access data within it\n",
    "thisclosure = closure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meshing segments with `ribbon_mesh` flag > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celeri.celeri_util import sph2cart, cart2sph\n",
    "import gmsh\n",
    "import meshio\n",
    "\n",
    "# Get indices/coordinates of segments with ribbon_mesh flag\n",
    "seg_mesh_idx = np.where(segment.create_ribbon_mesh > 0)[0]\n",
    "\n",
    "# Break if no segments need meshing\n",
    "if len(seg_mesh_idx) == 0:\n",
    "    print(\"No segments with create_ribbon_mesh > 0\")\n",
    "else:\n",
    "    # Calculate bottom coordinates\n",
    "    width_projected = segment.locking_depth/np.tan(np.deg2rad(segment.dip))\n",
    "    lon1_bot = np.zeros(len(segment))\n",
    "    lon2_bot = np.zeros(len(segment))\n",
    "    lat1_bot = np.zeros(len(segment))\n",
    "    lat2_bot = np.zeros(len(segment))\n",
    "\n",
    "    for i in range(len(segment)):\n",
    "        lon1_bot[i],lat1_bot[i], _ = GEOID.fwd(segment.lon1[i], segment.lat1[i], segment.azimuth[i]+90, 1e3*width_projected[i])\n",
    "        lon2_bot[i],lat2_bot[i], _ = GEOID.fwd(segment.lon2[i], segment.lat2[i], segment.azimuth[i]+90, 1e3*width_projected[i])\n",
    "\n",
    "    # Get block labels of the segments that should be meshes\n",
    "    sm_west_label = segment.loc[seg_mesh_idx, \"west_labels\"]\n",
    "    sm_east_label = segment.loc[seg_mesh_idx, \"east_labels\"]\n",
    "    sm_block_labels = np.sort(np.array([sm_east_label, sm_west_label]), axis=0)\n",
    "\n",
    "    # Unique blocks\n",
    "    sm_block_labels_unique, sm_block_labels_unique_idx = np.unique(sm_block_labels, axis=1, return_inverse=True)\n",
    "\n",
    "    # Loop through unique blocks and find indices of ordered coordinates\n",
    "    for i in range(np.shape(sm_block_labels_unique)[0]):\n",
    "        # Find the segments associated with this block\n",
    "        this_seg_mesh_idx = seg_mesh_idx[sm_block_labels_unique_idx == i]\n",
    "        # Get the ordered coordinates from the closure array, using the first block label\n",
    "        \n",
    "        # Concatenated endpoint arrays\n",
    "        this_coord1 = np.array([segment.loc[this_seg_mesh_idx, \"lon1\"], segment.loc[this_seg_mesh_idx, \"lat1\"]])\n",
    "        this_coord2 = np.array([segment.loc[this_seg_mesh_idx, \"lon2\"], segment.loc[this_seg_mesh_idx, \"lat2\"]])\n",
    "        seg_coords = np.zeros((2*len(this_seg_mesh_idx), 2))\n",
    "        seg_coords[0::2, :] = this_coord1.T\n",
    "        seg_coords[1::2, :] = this_coord2.T\n",
    "        seg_coords_bot = np.zeros((2*len(this_seg_mesh_idx), 3))\n",
    "        seg_coords_bot[0::2, :] = np.array([lon1_bot[this_seg_mesh_idx], lat1_bot[this_seg_mesh_idx], segment.loc[this_seg_mesh_idx, \"locking_depth\"]]).T\n",
    "        seg_coords_bot[1::2, :] = np.array([lon2_bot[this_seg_mesh_idx], lat2_bot[this_seg_mesh_idx], segment.loc[this_seg_mesh_idx, \"locking_depth\"]]).T\n",
    "        # Ordered coordinates from block closure\n",
    "        block_coords = thisclosure.polygons[sm_block_labels_unique[0, i]].vertices\n",
    "        # Find the indices. This is \n",
    "        seg_in_block_idx = np.unique(np.nonzero(np.all(block_coords == seg_coords[:,np.newaxis], axis=2))[1])\n",
    "        ordered_coords = block_coords[seg_in_block_idx, :]\n",
    "        # Ordered segment indices, needed to get averaged bottom coordinates\n",
    "        ordered_seg_idx = np.nonzero(np.all(seg_coords == ordered_coords[:,np.newaxis], axis=2))[1]\n",
    "        # Bottom indices 1 are first, odds, and last\n",
    "        bot_idx1 = np.zeros((len(ordered_coords), ))\n",
    "        bot_idx1[1:-1] = np.arange(1,len(ordered_seg_idx)-1,2)\n",
    "        bot_idx1[-1] = len(ordered_seg_idx)-1\n",
    "        # Bottom indices 2 are first, evens, and last\n",
    "        bot_idx2 = np.zeros((len(ordered_coords), ))\n",
    "        bot_idx2[1:-1] = np.arange(2,len(ordered_seg_idx)-1,2)\n",
    "        bot_idx2[-1] = len(ordered_seg_idx)-1\n",
    "        bot_coords1 = seg_coords_bot[ordered_seg_idx[bot_idx1.astype(int)], :]\n",
    "        bot_coords2 = seg_coords_bot[ordered_seg_idx[bot_idx2.astype(int)], :]\n",
    "        # Bottom coordinates are averages of \"internal\" endpoints (hanging ends are also averaged, but they're duplicates)\n",
    "        bot_coords = (bot_coords1 + bot_coords2)/2\n",
    "        # Top coordinates are ordered block coordinates with zero depths appended\n",
    "        top_coords = np.hstack((ordered_coords, np.zeros((len(ordered_coords),1))))\n",
    "        # Use top and bottom coordinates to make a mesh\n",
    "        filename = mesh_dir + '/' + seg_file_stem + '_ribbonmesh' + str(i+1)\n",
    "        clen = 5\n",
    "        \n",
    "        # Combined coordinates making a continuous perimeter loop\n",
    "        all_coords = np.vstack((top_coords, np.flipud(bot_coords)))\n",
    "\n",
    "        # Number of geometric objects\n",
    "        n_coords = np.shape(all_coords)[0]\n",
    "        n_surf = int((n_coords - 2) / 2)\n",
    "        n_lines = int(4 + (n_surf - 1)*3)\n",
    "\n",
    "        # Convert to Cartesian coordinates\n",
    "        cx, cy, cz = sph2cart(all_coords[:, 0], all_coords[:, 1], 6371-all_coords[:, 2])\n",
    "\n",
    "        if gmsh.isInitialized() == 0:\n",
    "            gmsh.initialize()\n",
    "        gmsh.option.setNumber(\"General.Verbosity\", 0)    \n",
    "        gmsh.clear()\n",
    "        # Define points\n",
    "        for j in range(n_coords):\n",
    "            gmsh.model.geo.addPoint(cx[j], cy[j], cz[j], clen, j)\n",
    "        # Define lines\n",
    "        # Start with lines around the perimeter\n",
    "        for j in range(n_coords-1):\n",
    "            gmsh.model.geo.addLine(j, j+1, j)\n",
    "        gmsh.model.geo.addLine(j+1, 0, j+1)\n",
    "        # Add interior lines\n",
    "        for k in range(n_surf - 1):\n",
    "            gmsh.model.geo.addLine(n_coords - k - 2, k+1, j+2+k)\n",
    "\n",
    "        # Define curve loops\n",
    "        # All but last\n",
    "        for m in range(n_surf-1):\n",
    "            gmsh.model.geo.addCurveLoop([m, -(j + 2 + m), j - m, j + 1 + m], m+1)\n",
    "        # Last\n",
    "        gmsh.model.geo.addCurveLoop([n_surf - 1, n_surf, n_surf + 1, j+2+k], m+2)\n",
    "        # Define surfaces\n",
    "        for m in range(n_surf):\n",
    "            gmsh.model.geo.addSurfaceFilling([m+1], m+1)\n",
    "        # Finish writing geo attributes\n",
    "        gmsh.model.geo.synchronize()\n",
    "        \n",
    "        # Combine interior panels\n",
    "        gmsh.model.mesh.setCompound(2, list(range(1, m+2)))\n",
    "    \n",
    "        # gmsh.write(filename + '.geo_unrolled')\n",
    "        \n",
    "        # Generate mesh\n",
    "        gmsh.model.mesh.generate(2)\n",
    "        # Access node coordinates and convert back to spherical \n",
    "        nodetags, nodecoords, _ = gmsh.model.mesh.getNodes(-1, -1)\n",
    "        lon, lat, r = cart2sph(nodecoords[0::3], nodecoords[1::3], nodecoords[2::3])\n",
    "        lon = np.rad2deg(lon)\n",
    "        lat = np.rad2deg(lat)\n",
    "        dep = r - 6371\n",
    "        nodecoords[0::3] = lon\n",
    "        nodecoords[1::3] = lat\n",
    "        nodecoords[2::3] = dep\n",
    "        # Reassign spherical node coordinates\n",
    "        for j in range(len(nodetags)):\n",
    "            gmsh.model.mesh.setNode(nodetags[j], nodecoords[3*j:3*j+3], [])\n",
    "        # Write the mesh for later reading in celeri \n",
    "        gmsh.write(filename + '.msh')\n",
    "        gmsh.finalize()  \n",
    "\n",
    "        # Update segment DataFrame\n",
    "        segment.loc[this_seg_mesh_idx, \"patch_file_name\"] = n_meshes + i # 0-based indexing means we start at n_meshes\n",
    "        segment.loc[this_seg_mesh_idx, \"patch_flag\"] = 1\n",
    "        segment.loc[this_seg_mesh_idx, \"create_ribbon_mesh\"] = 0\n",
    "\n",
    "        # Print status\n",
    "        print(\"Segments \" + np.array2string(this_seg_mesh_idx)  + \" meshed as \" + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating mesh parameters and command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish default mesh parameters\n",
    "mesh_default = {\n",
    "    \"smoothing_weight\": 1e0,\n",
    "    \"n_eigen\": 10,\n",
    "    \"top_slip_rate_constraint\": 0,\n",
    "    \"bot_slip_rate_constraint\": 0,\n",
    "    \"side_slip_rate_constraint\": 0,\n",
    "    \"top_slip_rate_weight\": 1,\n",
    "    \"bot_slip_rate_weight\": 1,\n",
    "    \"side_slip_rate_weight\": 1,\n",
    "    \"coupling_constraint_idx\": [],\n",
    "    \"ss_slip_constraint_idx\": [],\n",
    "    \"ss_slip_constraint_rate\": [],\n",
    "    \"ss_slip_constraint_sig\": [],\n",
    "    \"ss_slip_constraint_weight\": [],\n",
    "    \"ds_slip_constraint_idx\": [],\n",
    "    \"ds_slip_constraint_rate\": [],\n",
    "    \"ds_slip_constraint_sig\": [],\n",
    "    \"ds_slip_constraint_weight\": [],\n",
    "}\n",
    "\n",
    "# Assign all parameters to newly created meshes\n",
    "for j in range(i+1):\n",
    "    filename = mesh_dir + '/' + seg_file_stem + '_ribbonmesh' + str(j+1)\n",
    "    new_entry = {'mesh_filename': filename + '.msh'}\n",
    "    for key, value in mesh_default.items():\n",
    "        new_entry[key] = value\n",
    "    mesh_param.append(new_entry)\n",
    "\n",
    "# Write updated mesh_param json\n",
    "new_mesh_param_name = os.path.splitext(os.path.normpath(command.mesh_parameters_file_name))[0] + \"_ribbonmesh.json\"\n",
    "with open(new_mesh_param_name, \"w\") as mf:\n",
    "    json.dump(mesh_param, mf, indent=2) # indent=2 makes pretty json\n",
    "\n",
    "# Write updated segment csv\n",
    "new_segment_file_name = os.path.splitext(os.path.normpath(command.segment_file_name))[0] + \"_ribbonmesh.csv\"\n",
    "segment.to_csv(new_segment_file_name)\n",
    "\n",
    "# Write updated command json\n",
    "new_command_file_name = os.path.splitext(os.path.normpath(command_file_name))[0] + '_ribbonmesh.json'\n",
    "command[\"segment_file_name\"] = new_segment_file_name\n",
    "command[\"mesh_parameters_file_name\"] = new_mesh_param_name\n",
    "command[\"reuse_elastic\"] = 0\n",
    "with open(new_command_file_name, \"w\") as cf:\n",
    "    json.dump(command, cf, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a default plotting parameter dictionary\n",
    "estimation = {\"strike_slip_rates\": 0, \"dip_slip_rates\": 0, \"tensile_slip_rates\": 0}\n",
    "estimation = addict.Dict(estimation)\n",
    "p = celeri.get_default_plotting_dict(command, estimation, station)\n",
    "\n",
    "# Read in revised inputs\n",
    "command = celeri.get_command(new_command_file_name)\n",
    "segment, block, meshes, station, mogi, sar = celeri.read_data(command)\n",
    "celeri.plot_fault_geometry(p, segment, meshes)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b49ecae73d9755d1e6525bca8f2b993ba748c0e2e7c79677d2e15f06ac3538b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('celeri': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4d250c5d35aa493295ca814fb3eaa1ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6faf75ca5f3b41388f284e98ec2cf803": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.9.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_9b061db2dc65459ca586b9b9f73c2362",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle\nx/y fixes axis, CTRL fixes aspect",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "9b061db2dc65459ca586b9b9f73c2362": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c25a38234e8f4e818670d9767f95a430": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.9.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "_cursor": "default",
       "_figure_label": "Figure 1",
       "_height": 708,
       "_width": 1746,
       "layout": "IPY_MODEL_4d250c5d35aa493295ca814fb3eaa1ee",
       "toolbar": "IPY_MODEL_6faf75ca5f3b41388f284e98ec2cf803",
       "toolbar_position": "left"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
